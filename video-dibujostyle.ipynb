{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1 Operaciones Morfologicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall opencv-python opencv-python-headless -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Inicializar la captura de video desde la webcam (índice 0)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Definir el kernel\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "\n",
    "# Crear una ventana de visualización\n",
    "cv2.namedWindow('Estilizado en Tiempo Real', cv2.WINDOW_NORMAL)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    smoothed = cv2.bilateralFilter(frame, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "\n",
    "    # Redimensionar el marco para procesamiento más rápido\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "\n",
    "    # Convertir a escala de grises\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Aplicar cierre\n",
    "    closed = cv2.morphologyEx(gray, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # Aplicar apertura\n",
    "    opened = cv2.morphologyEx(closed, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    # Aplicar binarización\n",
    "    _, binary = cv2.threshold(opened, 120, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    # Calcular el gradiente morfológico\n",
    "    gradient = cv2.morphologyEx(opened, cv2.MORPH_GRADIENT, kernel)\n",
    "\n",
    "    # Invertir el gradiente para obtener bordes en blanco sobre fondo negro\n",
    "    gradient_inv = cv2.bitwise_not(gradient)\n",
    "\n",
    "    # Convertir el gradiente a tres canales\n",
    "    gradient_colored = cv2.cvtColor(gradient_inv, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # Aplicar el efecto de dibujo\n",
    "    styled_frame = cv2.bitwise_and(smoothed, gradient_colored)\n",
    "\n",
    "    # Mostrar el fotograma en proceso\n",
    "    cv2.imshow('Estilizado en Tiempo Real', styled_frame)\n",
    "\n",
    "    # Romper el ciclo con 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberar recursos\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Metrica de Operaciones Morfologicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  7.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Métrica de Cartoon-ness: 2.85%\n",
      "Máximo Cartoon-ness observado: 9.44%\n",
      "Imagen con máximo Cartoon-ness: bird.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "def calculate_cartoon_ness(image):\n",
    "    # Convertir a escala de grises\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Aplicar suavizado bilateral\n",
    "    smoothed = cv2.bilateralFilter(image, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "    \n",
    "    # Definir el kernel para operaciones morfológicas\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    \n",
    "    # Aplicar cierre y apertura\n",
    "    closed = cv2.morphologyEx(gray, cv2.MORPH_CLOSE, kernel)\n",
    "    opened = cv2.morphologyEx(closed, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    # Binarización\n",
    "    _, binary = cv2.threshold(opened, 120, 255, cv2.THRESH_BINARY_INV)\n",
    "    \n",
    "    # Calcular el gradiente morfológico\n",
    "    gradient = cv2.morphologyEx(opened, cv2.MORPH_GRADIENT, kernel)\n",
    "    \n",
    "    # Simplicidad de bordes\n",
    "    edge_length = np.sum(gradient > 0)\n",
    "    max_possible_edge_length = gradient.shape[0] * gradient.shape[1]\n",
    "    edge_simplicity = 1 - (edge_length / max_possible_edge_length)\n",
    "    \n",
    "    # Homogeneidad de regiones\n",
    "    _, labels, stats, _ = cv2.connectedComponentsWithStats(binary)\n",
    "    region_variances = []\n",
    "    for label in range(1, stats.shape[0]):\n",
    "        region = smoothed[labels == label]\n",
    "        if len(region) > 0:\n",
    "            region_variances.append(np.var(region))\n",
    "    avg_region_variance = np.mean(region_variances) if region_variances else 0\n",
    "    region_homogeneity = 1 / (1 + avg_region_variance)\n",
    "    \n",
    "    # Contraste de bordes\n",
    "    edge_mask = gradient > 0\n",
    "    edge_contrast = np.mean(smoothed[edge_mask]) - np.mean(smoothed[~edge_mask])\n",
    "    max_possible_contrast = 255\n",
    "    normalized_edge_contrast = edge_contrast / max_possible_contrast\n",
    "    \n",
    "    # Cálculo final de cartoon-ness\n",
    "    cartoon_ness = (0.4 * edge_simplicity + \n",
    "                    0.3 * region_homogeneity + \n",
    "                    0.3 * normalized_edge_contrast)\n",
    "    \n",
    "    return cartoon_ness\n",
    "\n",
    "# Obtener la ruta de la carpeta actual\n",
    "dataset_dir = os.path.join(os.getcwd(), \"imgs\")\n",
    "\n",
    "# Crear una carpeta para los resultados\n",
    "results_dir = \"resultados_cartoon_ness\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Generar un nombre de archivo único basado en la fecha y hora actual\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "csv_filename = os.path.join(results_dir, f\"resultados_cartoon_ness_{timestamp}.csv\")\n",
    "plot_filename = os.path.join(results_dir, f\"grafico_cartoon_ness_{timestamp}.png\")\n",
    "\n",
    "# Lista para almacenar los resultados\n",
    "results = []\n",
    "\n",
    "# Iterar sobre todas las imágenes en el directorio\n",
    "for filename in tqdm(os.listdir(dataset_dir)):\n",
    "    if filename.endswith((\".png\", \".jpg\", \".jpeg\",\".webp\")):\n",
    "        image_path = os.path.join(dataset_dir, filename)\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        if image is not None:\n",
    "            cartoon_ness = calculate_cartoon_ness(image)\n",
    "            results.append((filename, cartoon_ness))\n",
    "\n",
    "# Ordenar los resultados por cartoon-ness (de mayor a menor)\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Guardar los resultados en un archivo CSV\n",
    "with open(csv_filename, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(['Filename', 'Cartoon-ness'])  # Encabezados\n",
    "    for filename, score in results:\n",
    "        csvwriter.writerow([filename, f\"{score:.4f}\"])\n",
    "\n",
    "# Después de procesar todas las imágenes y antes de ordenar los resultados\n",
    "max_cartoon_ness = max(results, key=lambda x: x[1])\n",
    "max_cartoon_ness_filename, max_cartoon_ness_score = max_cartoon_ness\n",
    "\n",
    "# Calcular la métrica única\n",
    "scores = [score for _, score in results]\n",
    "cartoon_ness_metric = np.mean([score for _, score in results])\n",
    "cartoon_ness_percentage = cartoon_ness_metric * 100\n",
    "max_observed_cartoon_ness = max_cartoon_ness_score * 100\n",
    "\n",
    "# Añadir la métrica única, el máximo observado y la imagen correspondiente al CSV y al output\n",
    "with open(csv_filename, 'a', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(['Métrica de Cartoon-ness (%)', f\"{cartoon_ness_percentage:.2f}%\"])\n",
    "    csvwriter.writerow(['Máximo Cartoon-ness observado (%)', f\"{max_observed_cartoon_ness:.2f}%\"])\n",
    "    csvwriter.writerow(['Imagen con máximo Cartoon-ness', max_cartoon_ness_filename])\n",
    "\n",
    "print(f\"\\nMétrica de Cartoon-ness: {cartoon_ness_percentage:.2f}%\")\n",
    "print(f\"Máximo Cartoon-ness observado: {max_observed_cartoon_ness:.2f}%\")\n",
    "print(f\"Imagen con máximo Cartoon-ness: {max_cartoon_ness_filename}\")\n",
    "\n",
    "# Si quieres mostrar esta imagen en el gráfico\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(scores)), scores)\n",
    "plt.title('Puntuaciones de Cartoon-ness')\n",
    "plt.xlabel('Imágenes')\n",
    "plt.ylabel('Cartoon-ness')\n",
    "plt.annotate(f'Max: {max_cartoon_ness_filename}',\n",
    "            xy=(results.index(max_cartoon_ness), max_cartoon_ness_score),\n",
    "            xytext=(10, 10), textcoords='offset points',\n",
    "            arrowprops=dict(arrowstyle=\"->\"))\n",
    "plt.savefig(plot_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2 Yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar \n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRO TIP  Replace 'model=yolov5s.pt' with new 'model=yolov5su.pt'.\n",
      "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
      "\n",
      "\n",
      "0: 480x640 2 persons, 573.6ms\n",
      "Speed: 5.0ms preprocess, 573.6ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 3 chairs, 1 laptop, 599.7ms\n",
      "Speed: 6.0ms preprocess, 599.7ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 hot dog, 3 chairs, 2 tvs, 1 laptop, 542.0ms\n",
      "Speed: 3.0ms preprocess, 542.0ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 5 chairs, 1 laptop, 585.1ms\n",
      "Speed: 3.0ms preprocess, 585.1ms inference, 7.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 582.4ms\n",
      "Speed: 3.0ms preprocess, 582.4ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 551.4ms\n",
      "Speed: 4.0ms preprocess, 551.4ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 579.7ms\n",
      "Speed: 3.0ms preprocess, 579.7ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 585.2ms\n",
      "Speed: 3.0ms preprocess, 585.2ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 554.6ms\n",
      "Speed: 3.0ms preprocess, 554.6ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 553.1ms\n",
      "Speed: 2.0ms preprocess, 553.1ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 tv, 540.6ms\n",
      "Speed: 2.0ms preprocess, 540.6ms inference, 6.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 544.7ms\n",
      "Speed: 3.0ms preprocess, 544.7ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 toilet, 537.0ms\n",
      "Speed: 3.0ms preprocess, 537.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 520.7ms\n",
      "Speed: 3.0ms preprocess, 520.7ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 toilet, 529.0ms\n",
      "Speed: 3.0ms preprocess, 529.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 5 persons, 542.6ms\n",
      "Speed: 3.0ms preprocess, 542.6ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 toilet, 606.9ms\n",
      "Speed: 4.0ms preprocess, 606.9ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 674.0ms\n",
      "Speed: 3.0ms preprocess, 674.0ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 2 chairs, 609.0ms\n",
      "Speed: 5.0ms preprocess, 609.0ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 564.6ms\n",
      "Speed: 3.0ms preprocess, 564.6ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 572.2ms\n",
      "Speed: 3.0ms preprocess, 572.2ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 530.0ms\n",
      "Speed: 3.0ms preprocess, 530.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 541.6ms\n",
      "Speed: 3.0ms preprocess, 541.6ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 540.1ms\n",
      "Speed: 3.0ms preprocess, 540.1ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 573.4ms\n",
      "Speed: 3.0ms preprocess, 573.4ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 cell phone, 562.0ms\n",
      "Speed: 3.0ms preprocess, 562.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 elephant, 527.6ms\n",
      "Speed: 3.0ms preprocess, 527.6ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov5s.pt\") \n",
    "\n",
    "# Iniciar la captura de video\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Comprobar si la cámara está abierta\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: No se pudo abrir la cámara.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Realizar la detección de objetos\n",
    "    results = model(frame)\n",
    "    \n",
    "    # Procesar los resultados\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            # Convertir las coordenadas del tensor a una lista y luego a enteros\n",
    "            xyxy = box.xyxy[0].cpu().numpy()  # Convertir a numpy array y luego a enteros\n",
    "            x1, y1, x2, y2 = map(int, xyxy)  # Convertir a enteros\n",
    "            \n",
    "            # Obtener el índice de la clase y convertirlo a entero\n",
    "            class_idx = int(box.cls[0])\n",
    "            \n",
    "            # Obtener el nombre de la clase utilizando el índice\n",
    "            label = model.names[class_idx]\n",
    "            \n",
    "            # Dibujar la caja y la etiqueta en el frame\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    # Mostrar el video en tiempo real con las detecciones\n",
    "    cv2.imshow('Detección en Tiempo Real', frame)\n",
    "    \n",
    "    # Salir del bucle si se presiona 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberar la cámara y cerrar las ventanas\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRO TIP  Replace 'model=yolov5s.pt' with new 'model=yolov5su.pt'.\n",
      "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
      "\n",
      "\n",
      "0: 416x640 1 cat, 350.7ms\n",
      "Speed: 3.0ms preprocess, 350.7ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 320x640 (no detections), 288.7ms\n",
      "Speed: 2.0ms preprocess, 288.7ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n"
     ]
    }
   ],
   "source": [
    "#DETECCION DE IMAGENES MEDIANTE YOLO Y QUE SE GUARDEN COMO IMAGENES NUEVAS\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov5s.pt\") \n",
    "\n",
    "images_dir = \"imagenes\" \n",
    "image_files = os.listdir(images_dir)\n",
    "image_files = [os.path.join(images_dir, file) for file in image_files if file.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "for image_path in image_files:\n",
    "    frame = cv2.imread(image_path)\n",
    "    \n",
    "    if frame is None:\n",
    "        print(f\"Error al leer la imagen: {image_path}\")\n",
    "        continue\n",
    "    results = model(frame)\n",
    "    \n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            xyxy = box.xyxy[0].cpu().numpy() \n",
    "            x1, y1, x2, y2 = map(int, xyxy) \n",
    "            \n",
    "            # Obtener el índice de la clase y convertirlo a entero\n",
    "            class_idx = int(box.cls[0])\n",
    "            \n",
    "            # Obtener el nombre de la clase utilizando el índice\n",
    "            label = model.names[class_idx]\n",
    "            \n",
    "            # Dibujar la caja y la etiqueta en la imagen\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    # Mostrar la imagen con las detecciones\n",
    "    cv2.imshow('Detección en Imagen', frame)\n",
    "    \n",
    "    # Esperar a que se presione una tecla y cerrar la ventana\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRO TIP  Replace 'model=yolov5s.pt' with new 'model=yolov5su.pt'.\n",
      "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
      "\n",
      "\n",
      "0: 352x640 1 apple, 416.3ms\n",
      "Speed: 4.0ms preprocess, 416.3ms inference, 3.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 640x640 1 suitcase, 827.7ms\n",
      "Speed: 10.0ms preprocess, 827.7ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 umbrella, 836.0ms\n",
      "Speed: 9.0ms preprocess, 836.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bicycle, 916.9ms\n",
      "Speed: 9.0ms preprocess, 916.9ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x608 1 bird, 773.7ms\n",
      "Speed: 12.0ms preprocess, 773.7ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 640x416 1 bottle, 550.9ms\n",
      "Speed: 6.0ms preprocess, 550.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 512x640 1 cake, 643.2ms\n",
      "Speed: 5.6ms preprocess, 643.2ms inference, 3.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x640 (no detections), 723.2ms\n",
      "Speed: 7.0ms preprocess, 723.2ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 car, 563.8ms\n",
      "Speed: 3.0ms preprocess, 563.8ms inference, 4.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 1 cat, 928.6ms\n",
      "Speed: 10.0ms preprocess, 928.6ms inference, 7.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 384x640 1 dog, 801.5ms\n",
      "Speed: 4.5ms preprocess, 801.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 cup, 878.2ms\n",
      "Speed: 11.0ms preprocess, 878.2ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 suitcase, 804.7ms\n",
      "Speed: 7.9ms preprocess, 804.7ms inference, 4.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 airplane, 706.3ms\n",
      "Speed: 6.1ms preprocess, 706.3ms inference, 5.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 320x640 1 horse, 410.4ms\n",
      "Speed: 3.0ms preprocess, 410.4ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 640x640 2 airplanes, 847.2ms\n",
      "Speed: 7.0ms preprocess, 847.2ms inference, 7.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 remote, 981.6ms\n",
      "Speed: 7.0ms preprocess, 981.6ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 771.6ms\n",
      "Speed: 6.0ms preprocess, 771.6ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 airplane, 1 scissors, 932.5ms\n",
      "Speed: 9.0ms preprocess, 932.5ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 scissors, 791.1ms\n",
      "Speed: 5.0ms preprocess, 791.1ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 576x640 2 umbrellas, 854.0ms\n",
      "Speed: 19.0ms preprocess, 854.0ms inference, 4.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Métrica de precisión por clase:\n",
      "apple: 1.00\n",
      "suitcase: 0.00\n",
      "umbrella: 0.67\n",
      "bicycle: 1.00\n",
      "bird: 1.00\n",
      "bottle: 1.00\n",
      "cake: 0.00\n",
      "car: 1.00\n",
      "cat: 1.00\n",
      "dog: 1.00\n",
      "cup: 0.00\n",
      "airplane: 0.00\n",
      "horse: 1.00\n",
      "remote: 1.00\n",
      "scissors: 0.50\n",
      "\n",
      "Precisión promedio global: 0.68\n"
     ]
    }
   ],
   "source": [
    "#METRICA DE DETECCION DE OBJETOS\n",
    "import cv2\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov5s.pt\")\n",
    "\n",
    "images_dir = \"imagenes\"  \n",
    "image_files = os.listdir(images_dir)\n",
    "image_files = [os.path.join(images_dir, file) for file in image_files if file.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "class_counts = defaultdict(int)\n",
    "class_correct_counts = defaultdict(int)\n",
    "\n",
    "expected_labels = {}\n",
    "\n",
    "for image_path in image_files:\n",
    "    frame = cv2.imread(image_path)\n",
    "    \n",
    "    if frame is None:\n",
    "        print(f\"Error al leer la imagen: {image_path}\")\n",
    "        continue\n",
    "    \n",
    "    image_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    \n",
    "    expected_labels[image_name] = image_name\n",
    "    \n",
    "    results = model(frame)\n",
    "    \n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            xyxy = box.xyxy[0].cpu().numpy()  \n",
    "            x1, y1, x2, y2 = map(int, xyxy)  \n",
    "            class_idx = int(box.cls[0])\n",
    "            label = model.names[class_idx]\n",
    "            class_counts[label] += 1\n",
    "            if label == image_name:\n",
    "                class_correct_counts[label] += 1\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    cv2.imshow('Detección en Imagen', frame)\n",
    "    cv2.waitKey(100)\n",
    "cv2.destroyAllWindows()\n",
    "precision_per_class = {}\n",
    "\n",
    "for class_name in class_counts.keys():\n",
    "    if class_counts[class_name] > 0:\n",
    "        precision = class_correct_counts[class_name] / class_counts[class_name]\n",
    "    else:\n",
    "        precision = 0.0\n",
    "    precision_per_class[class_name] = precision\n",
    "\n",
    "total_images = len(image_files)\n",
    "total_precision = sum(precision_per_class.values()) / len(precision_per_class)\n",
    "\n",
    "print(\"Métrica de precisión por clase:\")\n",
    "for class_name, precision in precision_per_class.items():\n",
    "    print(f\"{class_name}: {precision:.2f}\")\n",
    "\n",
    "print(f\"\\nPrecisión promedio global: {total_precision:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3 Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install segmentation-models-pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade certifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --force-reinstall ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms  # Añadir esta línea\n",
    "\n",
    "# Descargar el modelo RESNet preentrenado\n",
    "model = models.segmentation.deeplabv3_resnet101(pretrained=True)\n",
    "model = model.eval()  # Establecer el modelo en modo de evaluación (no entrenamiento)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FCN_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=FCN_ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "\n",
    "# Descargar el modelo FCN-ResNet101 preentrenado\n",
    "model = models.segmentation.fcn_resnet101(pretrained=True)\n",
    "model = model.eval()  # Establecer el modelo en modo de evaluación (no entrenamiento)\n",
    "\n",
    "# Función para preprocesar la imagen capturada\n",
    "def preprocess_image(img):\n",
    "    # Redimensionar imagen a tamaño esperado por el modelo\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    img = transform(img)\n",
    "    img = img.unsqueeze(0)  # Agregar dimensión batch (1, C, H, W)\n",
    "    return img\n",
    "\n",
    "# Función para procesar el video desde la webcam\n",
    "def process_video(model):\n",
    "    cap = cv2.VideoCapture(0)  # Capturar video desde la primera cámara disponible\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Preprocesar el frame capturado\n",
    "        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img_tensor = preprocess_image(img)\n",
    "        \n",
    "        # Pasar la imagen por el modelo FCN-ResNet101\n",
    "        with torch.no_grad():\n",
    "            output = model(img_tensor)['out'][0]\n",
    "        \n",
    "        # Convertir la salida del modelo a imagen numpy y ajustar para visualización\n",
    "        output = output.argmax(0).byte().cpu().numpy()\n",
    "        \n",
    "        # Mostrar la imagen procesada en una ventana de OpenCV\n",
    "        cv2.imshow('Segmentación en tiempo real', output)\n",
    "        \n",
    "        # Salir del bucle con 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Llamar a la función para procesar el video\n",
    "process_video(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 79\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Llamar a la función para procesar un directorio de imágenes automáticamente\u001b[39;00m\n\u001b[0;32m     78\u001b[0m image_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimagenes\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Actualiza con la ruta a tu directorio de imágenes\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m \u001b[43mprocess_image_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 69\u001b[0m, in \u001b[0;36mprocess_image_directory\u001b[1;34m(directory, model)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# Mostrar la imagen segmentada y esperar una tecla\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSegmentación en Imagen\u001b[39m\u001b[38;5;124m'\u001b[39m, prediction)\n\u001b[1;32m---> 69\u001b[0m     \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Mostrar la imagen durante 1 segundo\u001b[39;00m\n\u001b[0;32m     71\u001b[0m cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Calcular la precisión promedio\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Cargar el modelo FCN-ResNet101 preentrenado\n",
    "model = models.segmentation.fcn_resnet101(pretrained=True)\n",
    "model = model.eval()\n",
    "\n",
    "# Función para preprocesar la imagen\n",
    "def preprocess_image(img):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    img = transform(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    return img\n",
    "\n",
    "# Directorio de imágenes\n",
    "images_dir = \"imagenes\"\n",
    "\n",
    "# Obtener lista de archivos de imagen\n",
    "image_files = [os.path.join(images_dir, file) for file in os.listdir(images_dir) \n",
    "               if file.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "class_counts = defaultdict(int)\n",
    "class_correct_pixels = defaultdict(int)\n",
    "class_total_pixels = defaultdict(int)\n",
    "\n",
    "# Procesar cada imagen\n",
    "for image_path in image_files:\n",
    "    frame = cv2.imread(image_path)\n",
    "    \n",
    "    if frame is None:\n",
    "        print(f\"Error al leer la imagen: {image_path}\")\n",
    "        continue\n",
    "    \n",
    "    image_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    \n",
    "    # Preprocesar y pasar la imagen por el modelo\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img_tensor = preprocess_image(img)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)['out'][0]\n",
    "    \n",
    "    # Convertir la salida del modelo a imagen numpy\n",
    "    output = output.argmax(0).byte().cpu().numpy()\n",
    "    \n",
    "    # Contar píxeles por clase\n",
    "    unique, counts = np.unique(output, return_counts=True)\n",
    "    for class_id, count in zip(unique, counts):\n",
    "        class_name = f\"class_{class_id}\"\n",
    "        class_counts[class_name] += 1\n",
    "        class_total_pixels[class_name] += count\n",
    "        \n",
    "        # Aquí deberías comparar con una máscara de verdad ground si la tienes\n",
    "        # Por ahora, asumimos que todos los píxeles son correctos (esto debe ser ajustado)\n",
    "        class_correct_pixels[class_name] += count\n",
    "    \n",
    "    # Visualizar resultados (opcional)\n",
    "    cv2.imshow('Segmentación', output * 10)  # Multiplicamos por 10 para mejor visualización\n",
    "    cv2.waitKey(100)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Calcular métricas\n",
    "iou_per_class = {}\n",
    "pixel_accuracy_per_class = {}\n",
    "\n",
    "for class_name in class_counts.keys():\n",
    "    if class_total_pixels[class_name] > 0:\n",
    "        iou = class_correct_pixels[class_name] / (class_total_pixels[class_name] + 1e-6)\n",
    "        pixel_accuracy = class_correct_pixels[class_name] / class_total_pixels[class_name]\n",
    "    else:\n",
    "        iou = 0.0\n",
    "        pixel_accuracy = 0.0\n",
    "    \n",
    "    iou_per_class[class_name] = iou\n",
    "    pixel_accuracy_per_class[class_name] = pixel_accuracy\n",
    "\n",
    "# Calcular métricas globales\n",
    "mean_iou = sum(iou_per_class.values()) / len(iou_per_class)\n",
    "mean_pixel_accuracy = sum(pixel_accuracy_per_class.values()) / len(pixel_accuracy_per_class)\n",
    "\n",
    "print(\"IoU por clase:\")\n",
    "for class_name, iou in iou_per_class.items():\n",
    "    print(f\"{class_name}: {iou:.4f}\")\n",
    "\n",
    "print(f\"\\nIoU promedio: {mean_iou:.4f}\")\n",
    "print(f\"Precisión de píxeles promedio: {mean_pixel_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos con imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Obtener la ruta de la carpeta actual\n",
    "current_folder = os.getcwd()\n",
    "imgs_folder = os.path.join(current_folder, 'imgs')\n",
    "\n",
    "# Función para aplicar un efecto de dibujo animado usando operaciones morfológicas\n",
    "def cartoonize_image_morph(image_path):\n",
    "    # Leer la imagen\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: no se puede leer la imagen {image_path}\")\n",
    "        return\n",
    "\n",
    "    # Convertir a escala de grises\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Aplicar la operación de gradiente morfológico\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "    gradient = cv2.morphologyEx(gray, cv2.MORPH_GRADIENT, kernel)\n",
    "\n",
    "    # Aplicar un umbral al gradiente\n",
    "    _, gradient_thresh = cv2.threshold(gradient, 40, 220, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Invertir el gradiente para obtener los bordes en blanco\n",
    "    gradient_inv = cv2.bitwise_not(gradient_thresh)\n",
    "    \n",
    "    # Expande el gradiente invertido a tres canales para que coincida con la imagen original\n",
    "    expanded_gradient_inv = cv2.merge([gradient_inv, gradient_inv, gradient_inv])\n",
    "\n",
    "    # Combinar la imagen original con los bordes\n",
    "    cartoon = cv2.bitwise_and(img, expanded_gradient_inv)\n",
    "\n",
    "    return cartoon\n",
    "\n",
    "# Procesar todas las imágenes en la carpeta 'imgs'\n",
    "for filename in os.listdir(imgs_folder):\n",
    "    # Filtrar solo archivos de imagen comunes\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.webp')):\n",
    "        input_path = os.path.join(imgs_folder, filename)\n",
    "        output_path_cartoon = os.path.join(current_folder, f'cartoon_{filename}')\n",
    "        \n",
    "        # Procesar la imagen\n",
    "        cartoon_image = cartoonize_image_morph(input_path)\n",
    "        \n",
    "        # Guardar la imagen procesada\n",
    "        if cartoon_image is not None:\n",
    "            cv2.imwrite(output_path_cartoon, cartoon_image)\n",
    "\n",
    "print(\"Procesamiento completado. Las imágenes se han guardado en la carpeta actual.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
