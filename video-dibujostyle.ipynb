{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1 Operaciones Morfologicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall opencv-python opencv-python-headless -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Inicializar la captura de video desde la webcam (índice 0)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Definir el kernel\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "\n",
    "# Crear una ventana de visualización\n",
    "cv2.namedWindow('Estilizado en Tiempo Real', cv2.WINDOW_NORMAL)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    smoothed = cv2.bilateralFilter(frame, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "\n",
    "    # Redimensionar el marco para procesamiento más rápido\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "\n",
    "    # Convertir a escala de grises\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Aplicar cierre\n",
    "    closed = cv2.morphologyEx(gray, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # Aplicar apertura\n",
    "    opened = cv2.morphologyEx(closed, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    # Aplicar binarización\n",
    "    _, binary = cv2.threshold(opened, 120, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    # Calcular el gradiente morfológico\n",
    "    gradient = cv2.morphologyEx(opened, cv2.MORPH_GRADIENT, kernel)\n",
    "\n",
    "    # Invertir el gradiente para obtener bordes en blanco sobre fondo negro\n",
    "    gradient_inv = cv2.bitwise_not(gradient)\n",
    "\n",
    "    # Convertir el gradiente a tres canales\n",
    "    gradient_colored = cv2.cvtColor(gradient_inv, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # Aplicar el efecto de dibujo\n",
    "    styled_frame = cv2.bitwise_and(smoothed, gradient_colored)\n",
    "\n",
    "    # Mostrar el fotograma en proceso\n",
    "    cv2.imshow('Estilizado en Tiempo Real', styled_frame)\n",
    "\n",
    "    # Romper el ciclo con 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberar recursos\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2 Yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar \n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRO TIP  Replace 'model=yolov5s.pt' with new 'model=yolov5su.pt'.\n",
      "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
      "\n",
      "\n",
      "0: 480x640 2 persons, 573.6ms\n",
      "Speed: 5.0ms preprocess, 573.6ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 3 chairs, 1 laptop, 599.7ms\n",
      "Speed: 6.0ms preprocess, 599.7ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 hot dog, 3 chairs, 2 tvs, 1 laptop, 542.0ms\n",
      "Speed: 3.0ms preprocess, 542.0ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 5 chairs, 1 laptop, 585.1ms\n",
      "Speed: 3.0ms preprocess, 585.1ms inference, 7.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 582.4ms\n",
      "Speed: 3.0ms preprocess, 582.4ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 551.4ms\n",
      "Speed: 4.0ms preprocess, 551.4ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 579.7ms\n",
      "Speed: 3.0ms preprocess, 579.7ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 585.2ms\n",
      "Speed: 3.0ms preprocess, 585.2ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 554.6ms\n",
      "Speed: 3.0ms preprocess, 554.6ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 553.1ms\n",
      "Speed: 2.0ms preprocess, 553.1ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 tv, 540.6ms\n",
      "Speed: 2.0ms preprocess, 540.6ms inference, 6.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tv, 544.7ms\n",
      "Speed: 3.0ms preprocess, 544.7ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 toilet, 537.0ms\n",
      "Speed: 3.0ms preprocess, 537.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 520.7ms\n",
      "Speed: 3.0ms preprocess, 520.7ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 toilet, 529.0ms\n",
      "Speed: 3.0ms preprocess, 529.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 5 persons, 542.6ms\n",
      "Speed: 3.0ms preprocess, 542.6ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 toilet, 606.9ms\n",
      "Speed: 4.0ms preprocess, 606.9ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 674.0ms\n",
      "Speed: 3.0ms preprocess, 674.0ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 2 chairs, 609.0ms\n",
      "Speed: 5.0ms preprocess, 609.0ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 chair, 564.6ms\n",
      "Speed: 3.0ms preprocess, 564.6ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 572.2ms\n",
      "Speed: 3.0ms preprocess, 572.2ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 530.0ms\n",
      "Speed: 3.0ms preprocess, 530.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 541.6ms\n",
      "Speed: 3.0ms preprocess, 541.6ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 540.1ms\n",
      "Speed: 3.0ms preprocess, 540.1ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 573.4ms\n",
      "Speed: 3.0ms preprocess, 573.4ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 cell phone, 562.0ms\n",
      "Speed: 3.0ms preprocess, 562.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 elephant, 527.6ms\n",
      "Speed: 3.0ms preprocess, 527.6ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov5s.pt\") \n",
    "\n",
    "# Iniciar la captura de video\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Comprobar si la cámara está abierta\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: No se pudo abrir la cámara.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Realizar la detección de objetos\n",
    "    results = model(frame)\n",
    "    \n",
    "    # Procesar los resultados\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            # Convertir las coordenadas del tensor a una lista y luego a enteros\n",
    "            xyxy = box.xyxy[0].cpu().numpy()  # Convertir a numpy array y luego a enteros\n",
    "            x1, y1, x2, y2 = map(int, xyxy)  # Convertir a enteros\n",
    "            \n",
    "            # Obtener el índice de la clase y convertirlo a entero\n",
    "            class_idx = int(box.cls[0])\n",
    "            \n",
    "            # Obtener el nombre de la clase utilizando el índice\n",
    "            label = model.names[class_idx]\n",
    "            \n",
    "            # Dibujar la caja y la etiqueta en el frame\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    # Mostrar el video en tiempo real con las detecciones\n",
    "    cv2.imshow('Detección en Tiempo Real', frame)\n",
    "    \n",
    "    # Salir del bucle si se presiona 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberar la cámara y cerrar las ventanas\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRO TIP  Replace 'model=yolov5s.pt' with new 'model=yolov5su.pt'.\n",
      "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
      "\n",
      "\n",
      "0: 416x640 1 cat, 350.7ms\n",
      "Speed: 3.0ms preprocess, 350.7ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 320x640 (no detections), 288.7ms\n",
      "Speed: 2.0ms preprocess, 288.7ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n"
     ]
    }
   ],
   "source": [
    "#DETECCION DE IMAGENES MEDIANTE YOLO Y QUE SE GUARDEN COMO IMAGENES NUEVAS\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov5s.pt\") \n",
    "\n",
    "images_dir = \"imagenes\" \n",
    "image_files = os.listdir(images_dir)\n",
    "image_files = [os.path.join(images_dir, file) for file in image_files if file.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "for image_path in image_files:\n",
    "    frame = cv2.imread(image_path)\n",
    "    \n",
    "    if frame is None:\n",
    "        print(f\"Error al leer la imagen: {image_path}\")\n",
    "        continue\n",
    "    results = model(frame)\n",
    "    \n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            xyxy = box.xyxy[0].cpu().numpy() \n",
    "            x1, y1, x2, y2 = map(int, xyxy) \n",
    "            \n",
    "            # Obtener el índice de la clase y convertirlo a entero\n",
    "            class_idx = int(box.cls[0])\n",
    "            \n",
    "            # Obtener el nombre de la clase utilizando el índice\n",
    "            label = model.names[class_idx]\n",
    "            \n",
    "            # Dibujar la caja y la etiqueta en la imagen\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    # Mostrar la imagen con las detecciones\n",
    "    cv2.imshow('Detección en Imagen', frame)\n",
    "    \n",
    "    # Esperar a que se presione una tecla y cerrar la ventana\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRO TIP  Replace 'model=yolov5s.pt' with new 'model=yolov5su.pt'.\n",
      "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
      "\n",
      "\n",
      "0: 352x640 1 apple, 416.3ms\n",
      "Speed: 4.0ms preprocess, 416.3ms inference, 3.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 640x640 1 suitcase, 827.7ms\n",
      "Speed: 10.0ms preprocess, 827.7ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 umbrella, 836.0ms\n",
      "Speed: 9.0ms preprocess, 836.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bicycle, 916.9ms\n",
      "Speed: 9.0ms preprocess, 916.9ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x608 1 bird, 773.7ms\n",
      "Speed: 12.0ms preprocess, 773.7ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 640x416 1 bottle, 550.9ms\n",
      "Speed: 6.0ms preprocess, 550.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 512x640 1 cake, 643.2ms\n",
      "Speed: 5.6ms preprocess, 643.2ms inference, 3.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x640 (no detections), 723.2ms\n",
      "Speed: 7.0ms preprocess, 723.2ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 car, 563.8ms\n",
      "Speed: 3.0ms preprocess, 563.8ms inference, 4.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 1 cat, 928.6ms\n",
      "Speed: 10.0ms preprocess, 928.6ms inference, 7.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 384x640 1 dog, 801.5ms\n",
      "Speed: 4.5ms preprocess, 801.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 cup, 878.2ms\n",
      "Speed: 11.0ms preprocess, 878.2ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 suitcase, 804.7ms\n",
      "Speed: 7.9ms preprocess, 804.7ms inference, 4.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 airplane, 706.3ms\n",
      "Speed: 6.1ms preprocess, 706.3ms inference, 5.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 320x640 1 horse, 410.4ms\n",
      "Speed: 3.0ms preprocess, 410.4ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 640x640 2 airplanes, 847.2ms\n",
      "Speed: 7.0ms preprocess, 847.2ms inference, 7.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 remote, 981.6ms\n",
      "Speed: 7.0ms preprocess, 981.6ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 (no detections), 771.6ms\n",
      "Speed: 6.0ms preprocess, 771.6ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 airplane, 1 scissors, 932.5ms\n",
      "Speed: 9.0ms preprocess, 932.5ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 scissors, 791.1ms\n",
      "Speed: 5.0ms preprocess, 791.1ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 576x640 2 umbrellas, 854.0ms\n",
      "Speed: 19.0ms preprocess, 854.0ms inference, 4.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Métrica de precisión por clase:\n",
      "apple: 1.00\n",
      "suitcase: 0.00\n",
      "umbrella: 0.67\n",
      "bicycle: 1.00\n",
      "bird: 1.00\n",
      "bottle: 1.00\n",
      "cake: 0.00\n",
      "car: 1.00\n",
      "cat: 1.00\n",
      "dog: 1.00\n",
      "cup: 0.00\n",
      "airplane: 0.00\n",
      "horse: 1.00\n",
      "remote: 1.00\n",
      "scissors: 0.50\n",
      "\n",
      "Precisión promedio global: 0.68\n"
     ]
    }
   ],
   "source": [
    "#METRICA DE DETECCION DE OBJETOS\n",
    "import cv2\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov5s.pt\")\n",
    "\n",
    "images_dir = \"imagenes\"  \n",
    "image_files = os.listdir(images_dir)\n",
    "image_files = [os.path.join(images_dir, file) for file in image_files if file.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "class_counts = defaultdict(int)\n",
    "class_correct_counts = defaultdict(int)\n",
    "\n",
    "expected_labels = {}\n",
    "\n",
    "for image_path in image_files:\n",
    "    frame = cv2.imread(image_path)\n",
    "    \n",
    "    if frame is None:\n",
    "        print(f\"Error al leer la imagen: {image_path}\")\n",
    "        continue\n",
    "    \n",
    "    image_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    \n",
    "    expected_labels[image_name] = image_name\n",
    "    \n",
    "    results = model(frame)\n",
    "    \n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            xyxy = box.xyxy[0].cpu().numpy()  \n",
    "            x1, y1, x2, y2 = map(int, xyxy)  \n",
    "            class_idx = int(box.cls[0])\n",
    "            label = model.names[class_idx]\n",
    "            class_counts[label] += 1\n",
    "            if label == image_name:\n",
    "                class_correct_counts[label] += 1\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    cv2.imshow('Detección en Imagen', frame)\n",
    "    cv2.waitKey(100)\n",
    "cv2.destroyAllWindows()\n",
    "precision_per_class = {}\n",
    "\n",
    "for class_name in class_counts.keys():\n",
    "    if class_counts[class_name] > 0:\n",
    "        precision = class_correct_counts[class_name] / class_counts[class_name]\n",
    "    else:\n",
    "        precision = 0.0\n",
    "    precision_per_class[class_name] = precision\n",
    "\n",
    "total_images = len(image_files)\n",
    "total_precision = sum(precision_per_class.values()) / len(precision_per_class)\n",
    "\n",
    "print(\"Métrica de precisión por clase:\")\n",
    "for class_name, precision in precision_per_class.items():\n",
    "    print(f\"{class_name}: {precision:.2f}\")\n",
    "\n",
    "print(f\"\\nPrecisión promedio global: {total_precision:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3 Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install segmentation-models-pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade certifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --force-reinstall ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms  # Añadir esta línea\n",
    "\n",
    "# Descargar el modelo RESNet preentrenado\n",
    "model = models.segmentation.deeplabv3_resnet101(pretrained=True)\n",
    "model = model.eval()  # Establecer el modelo en modo de evaluación (no entrenamiento)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FCN_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=FCN_ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "\n",
    "# Descargar el modelo FCN-ResNet101 preentrenado\n",
    "model = models.segmentation.fcn_resnet101(pretrained=True)\n",
    "model = model.eval()  # Establecer el modelo en modo de evaluación (no entrenamiento)\n",
    "\n",
    "# Función para preprocesar la imagen capturada\n",
    "def preprocess_image(img):\n",
    "    # Redimensionar imagen a tamaño esperado por el modelo\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    img = transform(img)\n",
    "    img = img.unsqueeze(0)  # Agregar dimensión batch (1, C, H, W)\n",
    "    return img\n",
    "\n",
    "# Función para procesar el video desde la webcam\n",
    "def process_video(model):\n",
    "    cap = cv2.VideoCapture(0)  # Capturar video desde la primera cámara disponible\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Preprocesar el frame capturado\n",
    "        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img_tensor = preprocess_image(img)\n",
    "        \n",
    "        # Pasar la imagen por el modelo FCN-ResNet101\n",
    "        with torch.no_grad():\n",
    "            output = model(img_tensor)['out'][0]\n",
    "        \n",
    "        # Convertir la salida del modelo a imagen numpy y ajustar para visualización\n",
    "        output = output.argmax(0).byte().cpu().numpy()\n",
    "        \n",
    "        # Mostrar la imagen procesada en una ventana de OpenCV\n",
    "        cv2.imshow('Segmentación en tiempo real', output)\n",
    "        \n",
    "        # Salir del bucle con 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Llamar a la función para procesar el video\n",
    "process_video(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 79\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Llamar a la función para procesar un directorio de imágenes automáticamente\u001b[39;00m\n\u001b[0;32m     78\u001b[0m image_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimagenes\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Actualiza con la ruta a tu directorio de imágenes\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m \u001b[43mprocess_image_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 69\u001b[0m, in \u001b[0;36mprocess_image_directory\u001b[1;34m(directory, model)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# Mostrar la imagen segmentada y esperar una tecla\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSegmentación en Imagen\u001b[39m\u001b[38;5;124m'\u001b[39m, prediction)\n\u001b[1;32m---> 69\u001b[0m     \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Mostrar la imagen durante 1 segundo\u001b[39;00m\n\u001b[0;32m     71\u001b[0m cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Calcular la precisión promedio\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#METRICA DE SEGMENTACIÓN\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Descargar el modelo FCN-ResNet101 preentrenado\n",
    "model = models.segmentation.fcn_resnet101(pretrained=True)\n",
    "model = model.eval()  # Establecer el modelo en modo de evaluación (no entrenamiento)\n",
    "\n",
    "# Función para preprocesar la imagen\n",
    "def preprocess_image(img):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    img = transform(img)\n",
    "    img = img.unsqueeze(0)  # Agregar dimensión batch (1, C, H, W)\n",
    "    return img\n",
    "\n",
    "# Función para procesar una imagen y obtener la segmentación\n",
    "def process_image(image_path, model):\n",
    "    # Leer la imagen desde el archivo\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # Preprocesar la imagen\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # OpenCV lee en BGR, convertir a RGB\n",
    "    img_tensor = preprocess_image(img)\n",
    "    \n",
    "    # Pasar la imagen por el modelo FCN-ResNet101\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)['out']\n",
    "    \n",
    "    # Convertir la salida del modelo a imagen numpy y ajustar para visualización\n",
    "    output = torch.argmax(output.squeeze(), dim=0).cpu().numpy().astype(np.uint8)  # Convertir a uint8\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Función para calcular la precisión de la segmentación (a implementar según las etiquetas reales)\n",
    "def calculate_segmentation_accuracy(predictions, labels):\n",
    "    # Aquí deberías implementar la comparación entre 'predictions' y 'labels' si tienes las etiquetas reales\n",
    "    # Por ahora, dejaremos la precisión como 0.0 ya que no hay comparación implementada\n",
    "    accuracy = 0.0\n",
    "    return accuracy\n",
    "\n",
    "# Función para procesar un directorio de imágenes automáticamente\n",
    "def process_image_directory(directory, model):\n",
    "    image_files = [os.path.join(directory, file) for file in os.listdir(directory) \n",
    "                   if file.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    for image_path in image_files:\n",
    "        # Obtener la etiqueta real esperada (asumiendo que el nombre del archivo es la etiqueta)\n",
    "        label = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        \n",
    "        # Procesar la imagen para obtener la segmentación\n",
    "        prediction = process_image(image_path, model)\n",
    "        \n",
    "        # Calcular la precisión de la segmentación (por implementar según las etiquetas reales)\n",
    "        accuracy = 0.0  # Por implementar correctamente la comparación con 'label'\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        # Mostrar la imagen segmentada y esperar una tecla\n",
    "        cv2.imshow('Segmentación en Imagen', prediction)\n",
    "        cv2.waitKey(1000)  # Mostrar la imagen durante 1 segundo\n",
    "        \n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Calcular la precisión promedio\n",
    "    average_accuracy = np.mean(accuracies)\n",
    "    print(f\"Precisión promedio de segmentación: {average_accuracy:.2f}\")\n",
    "\n",
    "# Llamar a la función para procesar un directorio de imágenes automáticamente\n",
    "image_directory = \"imagenes\"  # Actualiza con la ruta a tu directorio de imágenes\n",
    "process_image_directory(image_directory, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos con imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Obtener la ruta de la carpeta actual\n",
    "current_folder = os.getcwd()\n",
    "imgs_folder = os.path.join(current_folder, 'imgs')\n",
    "\n",
    "# Función para aplicar un efecto de dibujo animado usando operaciones morfológicas\n",
    "def cartoonize_image_morph(image_path):\n",
    "    # Leer la imagen\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: no se puede leer la imagen {image_path}\")\n",
    "        return\n",
    "\n",
    "    # Convertir a escala de grises\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Aplicar la operación de gradiente morfológico\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "    gradient = cv2.morphologyEx(gray, cv2.MORPH_GRADIENT, kernel)\n",
    "\n",
    "    # Aplicar un umbral al gradiente\n",
    "    _, gradient_thresh = cv2.threshold(gradient, 40, 220, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Invertir el gradiente para obtener los bordes en blanco\n",
    "    gradient_inv = cv2.bitwise_not(gradient_thresh)\n",
    "    \n",
    "    # Expande el gradiente invertido a tres canales para que coincida con la imagen original\n",
    "    expanded_gradient_inv = cv2.merge([gradient_inv, gradient_inv, gradient_inv])\n",
    "\n",
    "    # Combinar la imagen original con los bordes\n",
    "    cartoon = cv2.bitwise_and(img, expanded_gradient_inv)\n",
    "\n",
    "    return cartoon\n",
    "\n",
    "# Procesar todas las imágenes en la carpeta 'imgs'\n",
    "for filename in os.listdir(imgs_folder):\n",
    "    # Filtrar solo archivos de imagen comunes\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.webp')):\n",
    "        input_path = os.path.join(imgs_folder, filename)\n",
    "        output_path_cartoon = os.path.join(current_folder, f'cartoon_{filename}')\n",
    "        \n",
    "        # Procesar la imagen\n",
    "        cartoon_image = cartoonize_image_morph(input_path)\n",
    "        \n",
    "        # Guardar la imagen procesada\n",
    "        if cartoon_image is not None:\n",
    "            cv2.imwrite(output_path_cartoon, cartoon_image)\n",
    "\n",
    "print(\"Procesamiento completado. Las imágenes se han guardado en la carpeta actual.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
